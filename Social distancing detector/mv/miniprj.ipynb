{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CV_CAP_PROP_FRAME_WIDTH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hetpo\\Desktop\\Projects\\MV\\mv\\miniprj.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hetpo/Desktop/Projects/MV/mv/miniprj.ipynb#W0sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m cap \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mVideoCapture(\u001b[39m'\u001b[39m\u001b[39mvideo.mp4\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hetpo/Desktop/Projects/MV/mv/miniprj.ipynb#W0sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m cap \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mVideoCapture(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hetpo/Desktop/Projects/MV/mv/miniprj.ipynb#W0sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m cap\u001b[39m.\u001b[39mset(CV_CAP_PROP_FRAME_WIDTH, \u001b[39m640\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hetpo/Desktop/Projects/MV/mv/miniprj.ipynb#W0sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m cap\u001b[39m.\u001b[39mset(CV_CAP_PROP_FRAME_WIDTH, \u001b[39m480\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hetpo/Desktop/Projects/MV/mv/miniprj.ipynb#W0sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m#cap = cv2.VideoCapture('http://192.168.43.1:8080//video')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hetpo/Desktop/Projects/MV/mv/miniprj.ipynb#W0sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hetpo/Desktop/Projects/MV/mv/miniprj.ipynb#W0sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m#Euclidean distance for each video\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CV_CAP_PROP_FRAME_WIDTH' is not defined"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "import math\n",
    "\n",
    "import time\n",
    "\n",
    "# Load Yolo Model\n",
    "\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolo3.cfg\")\n",
    "\n",
    "classes = [\"person\"]\n",
    "\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "np.random.uniform(0, 255, size=(len(classes),3))\n",
    "\n",
    "#Start video or live camera\n",
    "\n",
    "cap = cv2.VideoCapture('video.mp4')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "#cap = cv2.VideoCapture('http://192.168.43.1:8080//video')\n",
    "\n",
    "#Euclidean distance for each video\n",
    "\n",
    "def E_dist(p1, p2):\n",
    "\n",
    "    return ((p1[0] - p2[0]) ** 2 +  (p1[1] - p2[1]) ** 2) ** 0.5\n",
    "\n",
    "def isclose(p1, p2):\n",
    "\n",
    "    c_d = E_dist(p1, p2)\n",
    "\n",
    "    calib = (p1[1] + p2[1]) / 2\n",
    "\n",
    "    if 0 < c_d < 0.15 * calib:\n",
    "\n",
    "        return 1\n",
    "\n",
    "    elif 0 < c_d < 0.2 * calib:\n",
    "\n",
    "        return 2\n",
    "\n",
    "    else:\n",
    "\n",
    "        return 0\n",
    "\n",
    "    \n",
    "\n",
    "height,width=(None,None)\n",
    "\n",
    "q=0       \n",
    "\n",
    "#Start working on video or camera\n",
    "\n",
    "while(cap.isOpened()):\n",
    "\n",
    "    # Capture frame-by-frame\n",
    "\n",
    "    ret, img = cap.read()  \n",
    "\n",
    "    print(ret)\n",
    "\n",
    "    if not ret:\n",
    "\n",
    "        break\n",
    "\n",
    "    if width is None or height is None: \n",
    "\n",
    "        height,width=img.shape[:2]\n",
    "\n",
    "        q=width\n",
    "\n",
    "    #height, width, channels = img.shape\n",
    "\n",
    "    img =img[0:height, 0:q]\n",
    "\n",
    "    height,width=img.shape[:2]\n",
    "\n",
    "    # Detecting objects 0.00392\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(img,0.00392, (416, 416), (0,0,0), True, crop=False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    end=time.time()\n",
    "\n",
    "     # Showing informations on the screen\n",
    "\n",
    "    class_ids = []\n",
    "\n",
    "    confidences = []\n",
    "\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "\n",
    "        for detection in out:\n",
    "\n",
    "            scores = detection[5:]\n",
    "\n",
    "            class_id = np.argmax(scores)\n",
    "\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            #0.5 is the threshold for confidence\n",
    "\n",
    "            if confidence > 0.5:\n",
    "\n",
    "                # Object detected\n",
    "\n",
    "                #Purpose : Converts center coordinates to rectangle coordinates\n",
    "\n",
    "                # x, y = midpoint of box\n",
    "\n",
    "                center_x = int(detection[0] * width)\n",
    "\n",
    "                center_y = int(detection[1] * height)\n",
    "\n",
    "                 # w, h = width, height of the box\n",
    "\n",
    "                w = int(detection[2] * width)\n",
    "\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Rectangle coordinates\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "\n",
    "                confidences.append(float(confidence))\n",
    "\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.5)\n",
    "\n",
    "    #print(indexes)\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX    \n",
    "\n",
    "    if len(indexes)>0:        \n",
    "\n",
    "        status=list()        \n",
    "\n",
    "        idf = indexes.flatten()        \n",
    "\n",
    "        close_pair = list()        \n",
    "\n",
    "        s_close_pair = list()        \n",
    "\n",
    "        center = list()        \n",
    "\n",
    "        dist = list()        \n",
    "\n",
    "        for i in idf:            \n",
    "\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])            \n",
    "\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])            \n",
    "\n",
    "            center.append([int(x + w / 2), int(y + h / 2)])            \n",
    "\n",
    "            status.append(0)            \n",
    "\n",
    "        for i in range(len(center)):            \n",
    "\n",
    "            for j in range(len(center)):                \n",
    "\n",
    "                #compare the closeness of two values\n",
    "\n",
    "                g=isclose(center[i], center[j])                \n",
    "\n",
    "                if g ==1:                    \n",
    "\n",
    "                    close_pair.append([center[i],center[j]])                    \n",
    "\n",
    "                    status[i] = 1                    \n",
    "\n",
    "                    status[j] = 1                    \n",
    "\n",
    "                elif g == 2:                    \n",
    "\n",
    "                    s_close_pair.append([center[i], center[j]])                    \n",
    "\n",
    "                    if status[i] != 1:                        \n",
    "\n",
    "                        status[i] = 2                        \n",
    "\n",
    "                    if status[j] != 1:                        \n",
    "\n",
    "                        status[j] = 2\n",
    "\n",
    "        total_p = len(center)        \n",
    "\n",
    "        low_risk_p = status.count(2)        \n",
    "\n",
    "        high_risk_p = status.count(1)        \n",
    "\n",
    "        safe_p = status.count(0)        \n",
    "\n",
    "        kk = 0        \n",
    "    \n",
    "        for i in idf:            \n",
    "\n",
    "            sub_img = img[10:170, 10:width - 10]            \n",
    "\n",
    "            black_rect = np.ones(sub_img.shape, dtype=np.uint8)*0            \n",
    "\n",
    "            res = cv2.addWeighted(sub_img, 0.77, black_rect,0.23, 1.0)\n",
    "\n",
    "            img[10:170, 10:width - 10] = res           \n",
    "\n",
    "            \n",
    "\n",
    "            # adding text to image            \n",
    "\n",
    "                      #(image,text,org( X coordinate value, Y coordinate value),font,fontScale,color,thikness)\n",
    "\n",
    "            cv2.putText(img, \"Social Distancing Detection - During COVID19 \", (255, 45),font, 1, (255, 255, 255), 2)       \n",
    "\n",
    "            #image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "\n",
    "            cv2.rectangle(img, (20, 60), (625, 160), (170, 170, 170), 2)            \n",
    "\n",
    "            cv2.putText(img, \"Connecting lines shows closeness among people. \", (45, 80),font, 0.6, (255, 255, 0), 1)            \n",
    "\n",
    "            cv2.putText(img, \"YELLOW: CLOSE\", (45, 110),font, 0.5, (0, 255, 255), 1)            \n",
    "\n",
    "            cv2.putText(img, \"RED: VERY CLOSE\", (45, 130),font, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "            cv2.rectangle(img, (675, 60), (width -20, 160), (170, 170, 170), 2)            \n",
    "\n",
    "            cv2.putText(img, \"Bounding box shows the level of risk to the person.\",(685, 80),font, 0.6, (255, 255, 0), 1)           \n",
    "\n",
    "            \n",
    "\n",
    "            cv2.putText(img, \"DARK RED: HIGH RISK\", (685, 110),font, 0.5, (0, 0, 150), 1)      \n",
    "\n",
    "            cv2.putText(img, \"ORANGE: LOW RISK\", (685, 130),font, 0.5, (0, 120, 255), 1)\n",
    "\n",
    "            cv2.putText(img, \"GREEN: CONGRATULATIONS YOU ARE SAFE\", (685, 150),font, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "            tot_str = \"NUMBER OF PEOPLE: \" + str(total_p)            \n",
    "\n",
    "            high_str = \"RED ZONE: \" + str(high_risk_p)            \n",
    "\n",
    "            low_str = \"ORANGE ZONE: \" + str(low_risk_p)            \n",
    "\n",
    "            safe_str = \"GREEN ZONE: \" + str(safe_p)            \n",
    "\n",
    "            #image ROI\n",
    "\n",
    "            sub_img = img[height - 120:height-20, 0:500]\n",
    "\n",
    "            #cv2.imshow(\"sub_img\",sub_img)            \n",
    "\n",
    "            black_rect = np.ones(sub_img.shape, dtype=np.uint8) * 0\n",
    "\n",
    "            res = cv2.addWeighted(sub_img, 0.8, black_rect, 0.2, 1.0)\n",
    "\n",
    "            img[height - 120:height-20, 0:500] = res\n",
    "\n",
    "            cv2.putText(img, tot_str, (10, height - 75),font, 0.6, (255, 255, 255), 1)            \n",
    "\n",
    "            cv2.putText(img, safe_str, (300, height - 75),font, 0.6, (0, 255, 0), 1)            \n",
    "\n",
    "            cv2.putText(img, low_str, (10, height - 50),font, 0.6, (0, 120, 255), 1)            \n",
    "\n",
    "            cv2.putText(img, high_str, (300, height - 50),font, 0.6, (0, 0, 150), 1)\n",
    "\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])            \n",
    "\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])        \n",
    "\n",
    "            #color of the ractangle when is too close \n",
    "\n",
    "            if status[kk] == 1:                \n",
    "\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 150), 2)\n",
    "\n",
    "            elif status[kk] == 0:                \n",
    "\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            else:\n",
    "\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (0, 120, 255), 2)\n",
    "\n",
    "            kk += 1\n",
    "\n",
    "        for h in close_pair:            \n",
    "\n",
    "            cv2.line(img, tuple(h[0]), tuple(h[1]), (0, 0, 255), 2)         \n",
    "\n",
    "        for b in s_close_pair:\n",
    "\n",
    "            cv2.line(img, tuple(b[0]), tuple(b[1]), (0, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow('image',img)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\n",
    "        break\n",
    "\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    #FourCC code is passed as\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "\n",
    "    output = cv2.VideoWriter('output4.mp4',fourcc, 20.0, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    # cv2.imwrite(\"output1.mp4\",img)\n",
    "\n",
    "    # img = cv2.flip(img,0)\n",
    "\n",
    "    output.write(img)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# output.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# press 'q' to release the window.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f8913e1baf3993bfc9d80404710451d42dead4b72f5e3c1142f7beafb96e005"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
